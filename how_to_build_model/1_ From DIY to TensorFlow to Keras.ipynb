{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNnbYvPhaxyOfXalen3wvIZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ICqkAveN8sjb"},"source":["# From DIY to TensorFlow 1.x to TensorFlow 2.x \n","此處三個範例均為採用鳶尾花資料集的預測問題\n","1. All DIY (Hard Coding)\n","2. TensorFlow 1.x\n","3. TensorFlow 2.x (Keras API)"]},{"cell_type":"markdown","metadata":{"id":"mQOINdn384R5"},"source":["## 1. All DIY (Hard Coding)"]},{"cell_type":"code","metadata":{"id":"GIBd1Z808pJu"},"source":["import numpy as np\n","import pprint\n","from sklearn import datasets\n","from scipy import special\n","pp = pprint.PrettyPrinter(indent=4)\n","\n","def softmax(x,der=False):\n","  if der:\n","    return np.exp(x)*(np.sum(np.exp(x))-np.exp(x)) / np.power(np.sum(np.exp(x)),2)\n","  e = np.exp(x - np.max(x, axis=1).reshape((-1, 1)))\n","  return e / e.sum(axis=1).reshape((-1, 1))\n","\n","def sigmoid(x,der=False):\n","  if der:\n","    return x*(1-x)\n","  else:\n","    return special.expit(x)\n","\n","def generate_batch(data,batch_size):\n","  data_set = []\n","  batch = []\n","  for i in range(0,len(data)):\n","    if i!=0 and i%batch_size==0:\n","      data_set.append(np.asarray(batch))\n","      batch = []\n","    batch.append(data[i])\n","  return data_set\n","\n","def random_sample(input, output):\n","  validation_input = []\n","  validation_output = []\n","  for i in range(0,10):\n","    picked_number=np.random.randint(0,len(input))\n","    validation_input.append(input[picked_number])\n","    validation_output.append(output[picked_number])\n","    input = np.delete(input, picked_number, 0)\n","    output = np.delete(output, picked_number, 0)\n","  return validation_input,validation_output\n","\n","def int_to_vector(data,label_size):\n","  vector = np.zeros((len(data),label_size),dtype='f')\n","  for i,single in enumerate(data):\n","    vector[i][single]=1\n","  return vector\n","\n","feature_size = 4  # input layer\n","label_size = 3  # output layer\n","second_layer_size = 4  # hidden layer\n","\n","iris = datasets.load_iris()\n","input = iris['data'][:,:feature_size]\n","output = iris['target']\n","\n","np.random.seed(1)\n","\n","w1 = 2 * np.random.random((feature_size,second_layer_size))-1 #4,4\n","w2 = 2 * np.random.random((second_layer_size,label_size))-1 #4,3\n","epoch = 30000\n","\n","new_output=[]\n","for value in output:\n","  new_output.append([value])\n","output = np.asarray(new_output)\n","\n","validation_input,validation_output=random_sample(input,output)\n","for ep in range(0,epoch):\n","  label_vector = int_to_vector(output,label_size)\n","  l1 = input\n","  l2 = sigmoid(np.dot(l1,w1))\n","  l3 = softmax(np.dot(l2,w2))\n","  l3_error = label_vector-l3\n","  l3_delta = l3_error*softmax(l3,True)\n","  l2_error = l3_delta.dot(w2.T)\n","  l2_delta = l2_error*sigmoid(l2,True)\n","  w2 += l2.T.dot(l3_delta)\n","  w1 += l1.T.dot(l2_delta)\n","  if ep%500==0:\n","    print(np.mean(np.abs(l3_error)))\n","\n","pp.pprint([np.argmax(x) for x in l3])\n","pp.pprint(label_vector)\n","\n","def predict(validation_input,validation_output):\n","  l1 = validation_input\n","  l2 = sigmoid(np.dot(l1, w1))\n","  l3 = softmax(np.dot(l2, w2))\n","  print([np.argmax(x) for x in l3])\n","  print([x[0] for x in np.asarray(validation_output)])\n","\n","predict(validation_input,validation_output)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JzDAliSz9SOh"},"source":["## 2. TensorFlow 1.x"]},{"cell_type":"code","metadata":{"id":"X5yzSjp-9UP_"},"source":["# 強制指定使用 1.x 版\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","# import tensorflow as tf\n","from sklearn import datasets\n","import numpy as np\n","\n","def int_to_vector(data,label_size):\n","  vector = np.zeros((len(data),label_size),dtype='f')\n","  for i,single in enumerate(data):\n","    vector[i][single]=1\n","  return vector\n","\n","iris = datasets.load_iris()\n","input = iris['data']\n","output = iris['target']\n","feature_size = 4\n","second_layer_size = 4\n","label_size = 3\n","output=int_to_vector(output,label_size)\n","tf.set_random_seed(1)\n","train_inputs = tf.placeholder(tf.float32, shape=[None,feature_size])\n","train_labels = tf.placeholder(tf.float32, shape=[None,label_size])\n","w1 = tf.Variable(tf.random_uniform([feature_size, second_layer_size], -1.0, 1.0))\n","w2 = tf.Variable(tf.random_uniform([second_layer_size, label_size], -1.0, 1.0))\n","pass_1 = tf.nn.sigmoid(tf.matmul(train_inputs,w1))\n","vali = tf.matmul(pass_1,w2)\n","pass_2 = tf.nn.softmax(tf.matmul(pass_1,w2))\n","\n","loss = tf.reduce_mean(tf.abs(train_labels-pass_2))\n","session = tf.Session()\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n","init = tf.global_variables_initializer()\n","session.run(init)\n","\n","epoch = 30000\n","feed_dict = {train_inputs: input, train_labels: output}\n","for i in range(0,epoch):\n","    pred, mid, intt, _, cur_loss = session.run([pass_2,pass_1,train_inputs,optimizer, loss], feed_dict=feed_dict)\n","    if i%1000==0:\n","        print(cur_loss)\n","\n","print([np.argmax(x) for x in pred])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxxffDTMHA2E"},"source":["## 3. TensorFlow 2.x (Keras API)"]},{"cell_type":"code","metadata":{"id":"f5PzAGa48tFV"},"source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.utils import to_categorical\n","from sklearn import datasets\n","\n","# 載入資料集\n","iris = datasets.load_iris()\n","x = iris.data  # iris['data']\n","y = iris.target  # iris['target']\n","\n","# 分割成訓練和測試資料集 (直接切割是錯誤的!!!)\n","# x_train, y_train = x[:120], y[:120]     # 訓練資料前120筆\n","# x_test, y_test = x[120:], y[120:]      # 測試資料後30筆\n","\n","# 打亂資料集中的所有資料 (permutation函式: 隨機排列)\n","i = np.random.permutation(len(iris.data))\n","\n","# 分割成訓練和測試資料集 (依隨機順序)\n","x_train = x[i[:120]]   # 前120條資料\n","y_train = y[i[:120]]   # 前120條資料對應的花的型別\n","x_test = x[i[120:]]   # 最後30條資料\n","y_test = y[i[120:]]   # 最後30條資料對應的花的型別\n","\n","# 對輸出進行 onehot 編碼\n","y_train_onehot = to_categorical(y_train)\n","y_test_onehot = to_categorical(y_test)\n","\n","# 建立Keras的Sequential模型\n","model = Sequential()\n","model.add(Dense(6, input_shape=(4,), activation='relu'))\n","model.add(Dense(6, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","# model.summary()   # 顯示模型摘要資訊\n","\n","# 編譯模型\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# 訓練模型\n","model.fit(x_train, y_train_onehot, epochs=100, batch_size=5)\n","\n","# 評估模型\n","loss, accuracy = model.evaluate(x_test, y_test_onehot, verbose=0)\n","print('準確度 = {:.2f}'.format(accuracy))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4tG5nRT01cU"},"source":["## 練習"]}]}